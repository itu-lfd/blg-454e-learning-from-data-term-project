{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20204c0c-4a09-4d32-89b3-60c3bfcf7204",
   "metadata": {},
   "source": [
    "__Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e068e683-7cfa-4d86-8a7b-df15fc4413a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from lda import LDA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing._data import _handle_zeros_in_scale\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41c2fc-111c-4258-964a-f6c025a28fce",
   "metadata": {},
   "source": [
    "__Reading CSV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9c6a94-9ba7-42b8-8b87-5f616723f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/aps_failure_training_set.csv')\n",
    "df_test = pd.read_csv('data/aps_failure_test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63096efe-2698-4c72-83d4-f42ca247fd53",
   "metadata": {},
   "source": [
    "__Replace Nan Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4998509-5059-476d-b42a-ad9c59102743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['class'] = df_train['class'].replace(['pos','neg'],[1,0])\n",
    "df_train = df_train.replace('na',np.NaN)\n",
    "\n",
    "#df_test['class'] = df_test['class'].replace(['pos','neg'],[1,0])\n",
    "df_test = df_test.replace('na',np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb98ae-5477-45fd-9633-b56276f26327",
   "metadata": {},
   "source": [
    "__Deleting Features With Zero Variance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bfa3c5-062f-488a-8546-9dcf62c12736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature with zero variance is :  cd_000\n",
      "The feature with zero variance is :  cd_000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16000, 170)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.astype(float)\n",
    "for i in df_train:\n",
    "  if df_train[i].std() == 0:\n",
    "    df_train = df_train.drop([i],axis=1)\n",
    "    print('The feature with zero variance is : ',i)\n",
    "df_train.shape\n",
    "\n",
    "df_test = df_test.astype(float)\n",
    "for i in df_test:\n",
    "  if df_test[i].std() == 0:\n",
    "    df_test = df_test.drop([i],axis=1)\n",
    "    print('The feature with zero variance is : ',i)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23559924-47bb-41f6-94d2-284cf2831517",
   "metadata": {},
   "source": [
    "__Deleting Duplicates__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f45424d-b88e-424d-bc1d-3f31fcd1640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 171)\n",
      "(16000, 170)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop_duplicates(keep = 'first')\n",
    "df_train = df_train.T.drop_duplicates().T\n",
    "print(df_train.shape)\n",
    "\n",
    "df_test = df_test.drop_duplicates(keep = 'first')\n",
    "df_test = df_test.T.drop_duplicates().T\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854a9d8-fa33-463b-afa8-bd27a287f36e",
   "metadata": {},
   "source": [
    "__Calculating Missing Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c35f09-e17f-4612-9110-fbdf2e739ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_feature_count = dict(df_train.drop('class',axis=1).isnull().sum())\n",
    "missing_feature_count = dict(sorted(missing_feature_count.items(), key=lambda item:item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0089f3-5820-4360-9062-466a1e272cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to be eliminated :  ['br_000', 'bq_000', 'bp_000', 'bo_000', 'ab_000', 'cr_000', 'bn_000', 'bm_000', 'bl_000', 'bk_000', 'ad_000', 'cf_000', 'cg_000', 'ch_000', 'co_000', 'ct_000', 'cu_000', 'cv_000', 'cx_000', 'cy_000', 'cz_000', 'da_000', 'db_000', 'dc_000']\n",
      "Number of features to be eliminated :  24\n",
      "\n",
      "Features for median imputaton :  ['ec_00', 'cm_000', 'cl_000', 'ed_000', 'ak_000', 'ca_000', 'dm_000', 'df_000', 'dg_000', 'dh_000', 'dl_000', 'dj_000', 'dk_000', 'eb_000', 'di_000', 'ac_000', 'bx_000', 'cc_000', 'bd_000', 'ds_000', 'dt_000', 'dp_000', 'dq_000', 'dr_000', 'du_000', 'dv_000', 'bc_000', 'cp_000', 'de_000', 'do_000', 'dy_000', 'ef_000', 'ar_000', 'bz_000', 'dx_000', 'dz_000', 'ea_000', 'eg_000', 'be_000', 'dd_000', 'ce_000', 'ax_000', 'ae_000', 'af_000', 'av_000', 'bf_000', 'bs_000', 'cb_000', 'bu_000', 'bv_000', 'cq_000', 'dn_000', 'ba_000', 'ba_001', 'ba_002', 'ba_003', 'ba_004', 'ba_005', 'ba_006', 'ba_007', 'ba_008', 'ba_009', 'cn_000', 'cn_001', 'cn_002', 'cn_003', 'cn_004', 'cn_005', 'cn_006', 'cn_007', 'cn_008', 'cn_009', 'ag_000', 'ag_001', 'ag_002', 'ag_003', 'ag_004', 'ag_005', 'ag_006', 'ag_007', 'ag_008', 'ag_009', 'ay_000', 'ay_001', 'ay_002', 'ay_003', 'ay_004', 'ay_005', 'ay_006', 'ay_007', 'ay_008', 'ay_009', 'az_000', 'az_001', 'az_002', 'az_003', 'az_004', 'az_005', 'az_006', 'az_007', 'az_008', 'az_009', 'ee_000', 'ee_001', 'ee_002', 'ee_003', 'ee_004', 'ee_005', 'ee_006', 'ee_007', 'ee_008', 'ee_009', 'cs_000', 'cs_001', 'cs_002', 'cs_003', 'cs_004', 'cs_005', 'cs_006', 'cs_007', 'cs_008', 'cs_009', 'ah_000', 'bb_000', 'al_000', 'an_000', 'ap_000', 'bg_000', 'bh_000', 'ai_000', 'aj_000', 'am_0', 'as_000', 'at_000', 'au_000', 'ao_000', 'aq_000', 'bi_000', 'bj_000', 'by_000', 'ci_000', 'cj_000', 'ck_000', 'bt_000', 'id', 'aa_000']\n",
      "Number of features for median imputaton :  146\n"
     ]
    }
   ],
   "source": [
    "features_tobe_eliminated = []\n",
    "median_imp_features = []\n",
    "model_imp_features = []\n",
    "for i in missing_feature_count.keys():\n",
    "  percent = (missing_feature_count[i]/df_train.shape[0])\n",
    "  if percent > 0.20:\n",
    "    features_tobe_eliminated.append(i)\n",
    "  else:\n",
    "    median_imp_features.append(i)\n",
    "\n",
    "print(\"Features to be eliminated : \",features_tobe_eliminated)\n",
    "print(\"Number of features to be eliminated : \",len(features_tobe_eliminated))\n",
    "print(\"\\nFeatures for median imputaton : \",median_imp_features)\n",
    "print(\"Number of features for median imputaton : \",len(median_imp_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9a6ae-1656-41d1-b5b8-aee1c1569aa6",
   "metadata": {},
   "source": [
    "__Train & CV Split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501dddfe-797d-4f94-a368-f40d086224bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Train Data ==========\n",
      "(60000, 170)\n",
      "(60000,)\n",
      "========== Test Data ==========\n",
      "(16000, 170)\n"
     ]
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "X_train = df_train.drop('class',axis=1)\n",
    "y_train = df_train['class']\n",
    "\n",
    "X_test = df_test\n",
    "\n",
    "X_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print(10*'='+\" Train Data \"+10*'=')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(10*'='+\" Test Data \"+10*'=')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b76ea1-edf6-4c7f-886c-e45ec2e04bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Train Data ==========\n",
      "(60000, 146)\n",
      "========== Test Data ==========\n",
      "(16000, 146)\n"
     ]
    }
   ],
   "source": [
    "X_train_droped = X_train.drop(features_tobe_eliminated,axis=1)\n",
    "X_test_droped = X_test.drop(features_tobe_eliminated,axis=1)\n",
    "\n",
    "print(10*'='+\" Train Data \"+10*'=')\n",
    "print(X_train_droped.shape)\n",
    "print(10*'='+\" Test Data \"+10*'=')\n",
    "print(X_test_droped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55ad14-d925-42db-902b-9040c908fd1d",
   "metadata": {},
   "source": [
    "__Scaling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716fcd0f-0647-459e-8bb5-df41dc44f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, sparse\n",
    "from sklearn.utils.validation import check_array, FLOAT_DTYPES\n",
    "\n",
    "class myRobustScaler:\n",
    "    def __init__(self, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False):\n",
    "        self.with_centering = with_centering\n",
    "        self.with_scaling = with_scaling\n",
    "        self.quantile_range = quantile_range\n",
    "        self.unit_variance = unit_variance\n",
    "        self.copy = copy\n",
    "        self.center_ = None\n",
    "        self.scale_ = None\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = check_array(X, accept_sparse=\"csc\", dtype=FLOAT_DTYPES, force_all_finite=\"allow-nan\")\n",
    "\n",
    "        q_min, q_max = self.quantile_range\n",
    "        if not 0 <= q_min <= q_max <= 100:\n",
    "            raise ValueError(\"Invalid quantile range: %s\" % str(self.quantile_range))\n",
    "\n",
    "        if self.with_centering:\n",
    "            if sparse.issparse(X):\n",
    "                raise ValueError(\"Cannot center sparse matrices: use `with_centering=False` instead.\")\n",
    "            self.center_ = np.nanmedian(X, axis=0)\n",
    "        else:\n",
    "            self.center_ = None\n",
    "\n",
    "        if self.with_scaling:\n",
    "            quantiles = []\n",
    "            for feature_idx in range(X.shape[1]):\n",
    "                if sparse.issparse(X):\n",
    "                    column_nnz_data = X.data[X.indptr[feature_idx] : X.indptr[feature_idx + 1]]\n",
    "                    column_data = np.zeros(shape=X.shape[0], dtype=X.dtype)\n",
    "                    column_data[: len(column_nnz_data)] = column_nnz_data\n",
    "                else:\n",
    "                    column_data = X[:, feature_idx]\n",
    "\n",
    "                quantiles.append(np.nanpercentile(column_data, self.quantile_range))\n",
    "\n",
    "            quantiles = np.transpose(quantiles)\n",
    "\n",
    "            self.scale_ = quantiles[1] - quantiles[0]\n",
    "            self.scale_ = _handle_zeros_in_scale(self.scale_, copy=False)\n",
    "            if self.unit_variance:\n",
    "                adjust = stats.norm.ppf(q_max / 100.0) - stats.norm.ppf(q_min / 100.0)\n",
    "                self.scale_ = self.scale_ / adjust\n",
    "        else:\n",
    "            self.scale_ = None\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = check_array(X, accept_sparse=(\"csr\", \"csc\"), copy=self.copy, dtype=FLOAT_DTYPES, force_all_finite=\"allow-nan\")\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if self.with_scaling:\n",
    "                inplace_column_scale(X, 1.0 / self.scale_)\n",
    "        else:\n",
    "            if self.with_centering:\n",
    "                X -= self.center_\n",
    "            if self.with_scaling:\n",
    "                X /= self.scale_\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "scaler = myRobustScaler()\n",
    "\n",
    "# Fit the scaler to the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train_droped)\n",
    "\n",
    "# Transform the test data using the parameters learned from the training data\n",
    "X_test_scaled = scaler.transform(X_test_droped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8efdff-2bd2-4d2b-991a-4551519153a3",
   "metadata": {},
   "source": [
    "__Median Imputation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc1004e-bb03-4e0e-a8ec-169576e2a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedianImputer:\n",
    "    def __init__(self, strategy='mean'):\n",
    "        self.strategy = strategy\n",
    "        self.statistics_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.strategy not in ['mean', 'median']:\n",
    "            raise ValueError(\"Invalid strategy. Please use 'mean' or 'median'.\")\n",
    "\n",
    "        if self.strategy == 'mean':\n",
    "            self.statistics_ = np.nanmean(X, axis=0)\n",
    "        elif self.strategy == 'median':\n",
    "            self.statistics_ = np.nanmedian(X, axis=0)\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.statistics_ is None:\n",
    "            raise ValueError(\"Imputer has not been fitted. Call fit() first.\")\n",
    "\n",
    "        X_imputed = X.copy()\n",
    "        for i in range(X.shape[1]):\n",
    "            nan_mask = np.isnan(X[:, i])\n",
    "            if np.any(nan_mask):\n",
    "                X_imputed[nan_mask, i] = self.statistics_[i]\n",
    "\n",
    "        return X_imputed\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf28bbf-37fd-4355-8369-743021622ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imputer = MedianImputer(strategy='median')\n",
    "median_imputer.fit(X_train_scaled)\n",
    "X_train_median = median_imputer.transform(X_train_scaled)\n",
    "X_test_median = median_imputer.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f017e-c277-4a7b-ab48-5f45c73a9d48",
   "metadata": {},
   "source": [
    "__LDA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3edfa1a6-83fa-4bad-98cf-88c6e5bb52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=5)\n",
    "\n",
    "# Fit the LDA model with the normalized features and target variable\n",
    "lda.fit(X_train_median, y_train)\n",
    "\n",
    "# Transform the features using the fitted LDA model\n",
    "x_train_lda = np.real(lda.transform(X_train_median))\n",
    "x_test_lda = np.real(lda.transform(X_test_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25ca42-9877-49c1-b9e7-04d8111f8089",
   "metadata": {},
   "source": [
    "__Oversampling With SMOTE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4542400a-a3eb-490f-a767-0d8cc145b06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    64000\n",
       "0.0    59000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nearest_neighbour(X):\n",
    "    nbs = NearestNeighbors(n_neighbors=100, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "def SMOTE_100(X):\n",
    "    indices2 = nearest_neighbour(X)\n",
    "    matrix = []\n",
    "\n",
    "    for m in range(len(indices2)):\n",
    "        t = X[indices2[m]]\n",
    "        newt = pd.DataFrame(t)\n",
    "        matrix.append([])\n",
    "        for j in range(len(newt.columns)):\n",
    "            matrix[m].append(random.choice(newt[j]))\n",
    "    return matrix\n",
    "\n",
    "def apply_SMOTE(X_train, Y_train, num_iterations=1):\n",
    "    for _ in range(num_iterations):\n",
    "        unique, counts = np.unique(Y_train, return_counts=True)\n",
    "        minority_shape = dict(zip(unique, counts))[1]\n",
    "\n",
    "        if isinstance(X_train, pd.DataFrame):\n",
    "            x1 = np.ones((minority_shape, X_train.shape[1]))\n",
    "            x1 = [X_train.iloc[i] for i, v in enumerate(Y_train) if v == 1.0]\n",
    "            x1 = np.array(x1)\n",
    "        elif isinstance(X_train, np.ndarray):\n",
    "            x1 = np.ones((minority_shape, X_train.shape[1]))\n",
    "            x1 = X_train[Y_train == 1.0]\n",
    "\n",
    "        sampled_instances = SMOTE_100(x1)\n",
    "\n",
    "        X_train = np.concatenate((X_train, sampled_instances), axis=0)\n",
    "\n",
    "        y_sampled_instances = np.ones(minority_shape)\n",
    "        Y_train = np.concatenate((Y_train, y_sampled_instances), axis=0)\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "# Example usage:\n",
    "x_train_lda_final, y_train_final = apply_SMOTE(x_train_lda, y_train, num_iterations=6)\n",
    "pd.DataFrame(y_train_final).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d9a68-4bc6-45d4-bb00-5f47fb00675b",
   "metadata": {},
   "source": [
    "__BEST__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27aa2ea2-efdc-4560-8f55-92e4d156a709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'alpha': 0.0001, 'class_weight': None, 'loss': 'hinge', 'n_jobs': -1, 'penalty': 'l1', 'random_state': 42}\n",
      "Best Accuracy:  0.9589024390243901\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'loss': ['hinge', 'log_loss', 'modified_huber'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'n_jobs': [-1],\n",
    "    'random_state': [42],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "}\n",
    "\n",
    "\n",
    "# Create the SGDClassifier\n",
    "sgd_classifier = SGDClassifier()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=sgd_classifier, param_grid=param_grid, scoring='accuracy', n_jobs=-1, cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(x_train_lda_final, y_train_final)\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Accuracy: \", grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.predict(x_test_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd46701-fc6b-4ee8-bfa3-1720b7bf884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns=['class'])\n",
    "y_pred_df['id'] = df_test['id']\n",
    "y_pred_df['class'] = y_pred_df['class'].replace([1,0],['pos','neg'])\n",
    "y_pred_df.to_csv('predicted_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded1244-f67f-4cb6-95b6-2bd3c5c522a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
